{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPp7J9fH4NVPM/zygdK/T25",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hivani/-Question-Answering-system-using-an-LLM-/blob/main/steps_AI_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Advanced Chunking of Data***"
      ],
      "metadata": {
        "id": "C1uydgNft46m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load JSON Data\n",
        "with open('/content/scraped_data.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Initialize the Sentence Transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Function to chunk data based on semantic similarity\n",
        "def chunk_data(doc):\n",
        "    sentences = doc.split('. ')\n",
        "    embeddings = model.encode(sentences)\n",
        "    clusters = util.community_detection(embeddings, min_community_size=2)\n",
        "    chunks = [' '.join([sentences[idx] for idx in cluster]) for cluster in clusters]\n",
        "    return chunks\n",
        "\n",
        "# Chunk all documents\n",
        "all_chunks = []\n",
        "for entry in data:\n",
        "    chunks = chunk_data(entry['content'])\n",
        "    for chunk in chunks:\n",
        "        all_chunks.append({\n",
        "            'text': chunk,\n",
        "            'meta': {'web_link': entry.get('web_link', 'N/A')}\n",
        "        })\n",
        "\n",
        "print(f\"Total chunks created: {len(all_chunks)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "188sDufFkTvc",
        "outputId": "929a3003-fda1-4db9-b999-2578cbf9d861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks created: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Convert Data to Embedding Vectors***"
      ],
      "metadata": {
        "id": "pgK8K2eTuOB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text chunks to embedding vectors\n",
        "chunk_texts = [chunk['text'] for chunk in all_chunks]\n",
        "chunk_embeddings = model.encode(chunk_texts)\n"
      ],
      "metadata": {
        "id": "_OR3m_Afknld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install grpcio==1.63.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkhUlTsJnirG",
        "outputId": "285687e5-0f56-4e05-9286-abef8ea16654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: grpcio==1.63.0 in /usr/local/lib/python3.10/dist-packages (1.63.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TfcscY5pPgQ",
        "outputId": "c0d8f15b-6d65-4788-c981-450424b25abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create Vector Database **"
      ],
      "metadata": {
        "id": "5Wg2x4mKuTZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Convert the embeddings to float32\n",
        "chunk_embeddings = np.array(chunk_embeddings).astype('float32')\n",
        "\n",
        "# Create a FAISS index\n",
        "dimension = chunk_embeddings.shape[1]\n",
        "index = faiss.IndexHNSWFlat(dimension, 32)  # HNSW index with 32 neighbors\n",
        "index.hnsw.efConstruction = 200\n",
        "\n",
        "# Add embeddings to the index\n",
        "index.add(chunk_embeddings)\n",
        "\n",
        "# Store metadata (in-memory for simplicity, you may use a database or file)\n",
        "metadata = [{'web_link': chunk['meta']['web_link']} for chunk in all_chunks]\n"
      ],
      "metadata": {
        "id": "IzCuU6cakxZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ** Retrieval and Re-ranking**"
      ],
      "metadata": {
        "id": "2unQ39YKubdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a simple query expansion technique (e.g., synonyms)\n",
        "def expand_query(query):\n",
        "    synonyms = [\"synonym1\", \"synonym2\"]  # Example synonyms\n",
        "    expanded_query = f\"{query} {' '.join(synonyms)}\"\n",
        "    return expanded_query\n",
        "\n",
        "# Expand the query\n",
        "query = \"example query\"\n",
        "expanded_query = expand_query(query)\n"
      ],
      "metadata": {
        "id": "ofRgbnSxvEtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Use a simple query expansion technique (e.g., synonyms)\n",
        "def expand_query(query):\n",
        "    synonyms = [\"synonym1\", \"synonym2\"]  # Example synonyms\n",
        "    expanded_query = f\"{query} {' '.join(synonyms)}\"\n",
        "    return expanded_query\n",
        "\n",
        "# Expand the query\n",
        "query = \"example query\"\n",
        "expanded_query = expand_query(query)\n"
      ],
      "metadata": {
        "id": "6YewzHKbr7sM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hybrid Retrieval Methods**"
      ],
      "metadata": {
        "id": "Mbk2xNHTug6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Encode the query\n",
        "query_embedding = model.encode([expanded_query]).astype('float32')\n",
        "\n",
        "# Search the index\n",
        "k = 10  # Number of nearest neighbors\n",
        "distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "# Extract results\n",
        "retrieved_chunks = [chunk_texts[i] for i in indices[0]]\n",
        "retrieved_metadata = [metadata[i] for i in indices[0]]\n",
        "\n",
        "print(f\"Retrieved Chunks: {retrieved_chunks}\")\n",
        "print(f\"Retrieved Metadata: {retrieved_metadata}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rlhe67Dr87i",
        "outputId": "60dfaa09-44ea-4963-ebc6-f7adb771a9d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved Chunks: ['This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Hopper GPU Architectureâ\\x80\\x99s features.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"ada-tuning-guide/index.html\"><span class=\"doc\">Ada Tuning Guide</span></a></dt><dd><p>The NVIDIAÂ® Ada GPU architecture is NVIDIAâ\\x80\\x99s latest architecture for CUDAÂ® compute applications This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging NVIDIA Ampere GPU Architectureâ\\x80\\x99s features.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"hopper-tuning-guide/index.html\"><span class=\"doc\">Hopper Tuning Guide</span></a></dt><dd><p>Hopper GPU Architecture is NVIDIAâ\\x80\\x99s 9th-generation architecture for CUDA compute applications This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Turing architectural features.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"ampere-tuning-guide/index.html\"><span class=\"doc\">NVIDIA Ampere GPU Architecture Tuning Guide</span></a></dt><dd><p>NVIDIA Ampere GPU Architecture is NVIDIAâ\\x80\\x99s 8th-generation architecture for CUDA compute applications This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Maxwell architectural features.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"pascal-tuning-guide/index.html\"><span class=\"doc\">Pascal Tuning Guide</span></a></dt><dd><p>Pascal is NVIDIAâ\\x80\\x99s 5th-generation architecture for CUDA compute applications This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Pascal architectural features.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"volta-tuning-guide/index.html\"><span class=\"doc\">Volta Tuning Guide</span></a></dt><dd><p>Volta is NVIDIAâ\\x80\\x99s 6th-generation architecture for CUDA compute applications This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Volta architectural features.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"turing-tuning-guide/index.html\"><span class=\"doc\">Turing Tuning Guide</span></a></dt><dd><p>Turing is NVIDIAâ\\x80\\x99s 7th-generation architecture for CUDA compute applications Applications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the Hopper GPU Architecture without any code changes This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Ada GPU architectureâ\\x80\\x99s features.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"parallel-thread-execution/index.html\"><span class=\"doc\">PTX ISA</span></a></dt><dd><p>This guide provides detailed instructions on the use of PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA)', 'This document provides guidance to ensure that your software applications are compatible with NVIDIA Ampere GPU architecture.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"hopper-compatibility-guide/index.html\"><span class=\"doc\">Hopper Compatibility Guide</span></a></dt><dd><p>This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Hopper GPUs This document provides guidance to ensure that your software applications are compatible with Hopper architecture.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"ada-compatibility-guide/index.html\"><span class=\"doc\">Ada Compatibility Guide</span></a></dt><dd><p>This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Ada GPUs', 'This document provides guidance to ensure that your software applications are compatible with Volta.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"turing-compatibility-guide/index.html\"><span class=\"doc\">Turing Compatibility Guide</span></a></dt><dd><p>This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Turing Architecture This document provides guidance to ensure that your software applications are compatible with Turing.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"ampere-compatibility-guide/index.html\"><span class=\"doc\">NVIDIA Ampere GPU Architecture Compatibility Guide</span></a></dt><dd><p>This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Ampere GPU Architecture', 'This document provides guidance to ensure that your software applications are compatible with Maxwell.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"pascal-compatibility-guide/index.html\"><span class=\"doc\">Pascal Compatibility Guide</span></a></dt><dd><p>This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Pascal Architecture This document provides guidance to ensure that your software applications are compatible with Pascal.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"volta-compatibility-guide/index.html\"><span class=\"doc\">Volta Compatibility Guide</span></a></dt><dd><p>This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Volta Architecture This document provides guidance to ensure that your software applications are compatible with Ada architecture.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"maxwell-tuning-guide/index.html\"><span class=\"doc\">Maxwell Tuning Guide</span></a></dt><dd><p>Maxwell is NVIDIAâ\\x80\\x99s 4th-generation architecture for CUDA compute applications The intent is to provide guidelines for obtaining the best performance from NVIDIA GPUs using the CUDA Toolkit.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"maxwell-compatibility-guide/index.html\"><span class=\"doc\">Maxwell Compatibility Guide</span></a></dt><dd><p>This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Maxwell Architecture', 'Applications that follow the best practices for the Pascal architecture should typically see speedups on the Volta architecture without any code changes Applications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the NVIDIA Ampere GPU Architecture without any code changes Applications that follow the best practices for the Maxwell architecture should typically see speedups on the Pascal architecture without any code changes Applications that follow the best practices for the Pascal architecture should typically see speedups on the Turing architecture without any code changes', 'This document provides guidance to ensure that your software applications are compatible with NVIDIA Ampere GPU architecture.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"hopper-compatibility-guide/index.html\"><span class=\"doc\">Hopper Compatibility Guide</span></a></dt><dd><p>This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Hopper GPUs This document provides guidance to ensure that your software applications are compatible with Hopper architecture.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"ada-compatibility-guide/index.html\"><span class=\"doc\">Ada Compatibility Guide</span></a></dt><dd><p>This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Ada GPUs', 'This document provides guidance to ensure that your software applications are compatible with NVIDIA Ampere GPU architecture.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"hopper-compatibility-guide/index.html\"><span class=\"doc\">Hopper Compatibility Guide</span></a></dt><dd><p>This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Hopper GPUs This document provides guidance to ensure that your software applications are compatible with Hopper architecture.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"ada-compatibility-guide/index.html\"><span class=\"doc\">Ada Compatibility Guide</span></a></dt><dd><p>This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Ada GPUs', 'This document provides guidance to ensure that your software applications are compatible with NVIDIA Ampere GPU architecture.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"hopper-compatibility-guide/index.html\"><span class=\"doc\">Hopper Compatibility Guide</span></a></dt><dd><p>This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Hopper GPUs This document provides guidance to ensure that your software applications are compatible with Hopper architecture.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"ada-compatibility-guide/index.html\"><span class=\"doc\">Ada Compatibility Guide</span></a></dt><dd><p>This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Ada GPUs', 'This document provides guidance to ensure that your software applications are compatible with NVIDIA Ampere GPU architecture.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"hopper-compatibility-guide/index.html\"><span class=\"doc\">Hopper Compatibility Guide</span></a></dt><dd><p>This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Hopper GPUs This document provides guidance to ensure that your software applications are compatible with Hopper architecture.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"ada-compatibility-guide/index.html\"><span class=\"doc\">Ada Compatibility Guide</span></a></dt><dd><p>This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Ada GPUs', 'This document provides guidance to ensure that your software applications are compatible with NVIDIA Ampere GPU architecture.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"hopper-compatibility-guide/index.html\"><span class=\"doc\">Hopper Compatibility Guide</span></a></dt><dd><p>This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Hopper GPUs This document provides guidance to ensure that your software applications are compatible with Hopper architecture.</p>\\r\\n</dd>\\r\\n<dt><a class=\"reference internal\" href=\"ada-compatibility-guide/index.html\"><span class=\"doc\">Ada Compatibility Guide</span></a></dt><dd><p>This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Ada GPUs']\n",
            "Retrieved Metadata: [{'web_link': 'N/A'}, {'web_link': 'N/A'}, {'web_link': 'N/A'}, {'web_link': 'N/A'}, {'web_link': 'N/A'}, {'web_link': 'N/A'}, {'web_link': 'N/A'}, {'web_link': 'N/A'}, {'web_link': 'N/A'}, {'web_link': 'N/A'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-rank the retrieved results\n",
        "reranked_results = util.semantic_search(model.encode(query), model.encode(retrieved_chunks), top_k=10)\n",
        "\n",
        "# Flatten the re-ranked results\n",
        "reranked_chunks = [retrieved_chunks[idx['corpus_id']] for idx in reranked_results[0]]\n"
      ],
      "metadata": {
        "id": "WuzD-Pl4r_GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize QA pipeline\n",
        "qa_pipeline = pipeline('question-answering', model='deepset/roberta-base-squad2')\n"
      ],
      "metadata": {
        "id": "7i0lanazsCFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the top re-ranked result for question answering\n",
        "context = reranked_chunks[0]\n",
        "answer = qa_pipeline({'question': query, 'context': context})\n",
        "\n",
        "print(f\"Answer: {answer['answer']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX7_Vv-MsE72",
        "outputId": "c161cef1-352f-4e02-a4b6-3f59d78cf45e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: NVIDIA Ampere GPU Architecture\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2hlKge0vcwU",
        "outputId": "8e8955ae-7074-438d-ef3f-422f3379586c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.36.0-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<5,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.7.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.19.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Installing collected packages: watchdog, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.36.0 watchdog-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "# Streamlit UI for input query and displaying the answer\n",
        "st.title('Question Answering System')\n",
        "\n",
        "query = st.text_input('Enter your query:')\n",
        "if query:\n",
        "    expanded_query = expand_query(query)\n",
        "    query_embedding = model.encode([expanded_query]).astype('float32')\n",
        "    k = 10  # Number of nearest neighbors\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    # Extract results\n",
        "    retrieved_chunks = [chunk_texts[i] for i in indices[0]]\n",
        "    retrieved_metadata = [metadata[i] for i in indices[0]]\n",
        "\n",
        "    # Re-rank the retrieved results\n",
        "    reranked_results = util.semantic_search(model.encode(query), model.encode(retrieved_chunks), top_k=10)\n",
        "    reranked_chunks = [retrieved_chunks[idx['corpus_id']] for idx in reranked_results[0]]\n",
        "\n",
        "    # Use the top re-ranked result for question answering\n",
        "    context = reranked_chunks[0]\n",
        "    answer = qa_pipeline({'question': query, 'context': context})\n",
        "\n",
        "    st.write(f\"Answer: {answer['answer']}\")\n"
      ],
      "metadata": {
        "id": "ydG-0YfovY3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RcSksmoHwz4H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}